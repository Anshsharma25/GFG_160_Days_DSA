{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845f804a",
   "metadata": {},
   "source": [
    "Q1 : What is the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ac78ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDefinition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.\\u200b\\n\\nKey Characteristics:\\n\\nUnlabeled Data: Works with data that has no predefined labels or categories.\\n\\nObjective: Discover hidden patterns, groupings, or features in the data.\\n\\nCommon Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.\\u200b\\n\\nExamples:\\n\\nCustomer Segmentation: Group customers based on purchasing behavior without predefined categories.\\n\\nAnomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\\n\\nMarket Basket Analysis: Discover associations between products bought together.\\u200b\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#üìò Supervised Learning\n",
    "'''Definition: Supervised learning involves training a model on a labeled dataset, where each input is paired with the correct output. The model learns to predict the output from the input data.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Labeled Data: Requires a dataset with input-output pairs.\n",
    "\n",
    "Objective: Learn a mapping from inputs to outputs to make accurate predictions on new, unseen data.\n",
    "\n",
    "Common Algorithms: Linear regression, logistic regression, decision trees, support vector machines, neural networks.‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Examples:\n",
    "\n",
    "Email Spam Detection: Classify emails as 'spam' or 'not spam' based on labeled examples.\n",
    "\n",
    "Medical Diagnosis: Predict disease presence from patient data with known diagnoses.\n",
    "\n",
    "Credit Scoring: Assess loan applicants' creditworthiness using historical data with known outcomes '''\n",
    "\n",
    "\n",
    "# üìô Unsupervised Learning\n",
    "'''\n",
    "Definition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Unlabeled Data: Works with data that has no predefined labels or categories.\n",
    "\n",
    "Objective: Discover hidden patterns, groupings, or features in the data.\n",
    "\n",
    "Common Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.‚Äã\n",
    "\n",
    "Examples:\n",
    "\n",
    "Customer Segmentation: Group customers based on purchasing behavior without predefined categories.\n",
    "\n",
    "Anomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\n",
    "\n",
    "Market Basket Analysis: Discover associations between products bought together.‚Äã\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72a4c3",
   "metadata": {},
   "source": [
    "What is overfitting? How can you prevent it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d137fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \\nStack Overflow\\n.\\u200b\\n\\n‚úÖ How to Prevent Overfitting\\nTo mitigate overfitting, consider the following strategies:\\n\\nCross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .\\u200b\\nEliteDataScience\\n\\nRegularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .\\u200b\\n\\nEarly Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .\\u200b\\nV7\\n\\nSimplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .\\u200b\\n\\nIncrease Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .\\u200b\\n\\nData Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .\\u200b\\n\\nDropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .\\u200b\\nWikipedia\\n\\nPruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .\\u200b\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \n",
    "Stack Overflow\n",
    ".‚Äã\n",
    "\n",
    "‚úÖ How to Prevent Overfitting\n",
    "To mitigate overfitting, consider the following strategies:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .‚Äã\n",
    "EliteDataScience\n",
    "\n",
    "Regularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .‚Äã\n",
    "\n",
    "Early Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .‚Äã\n",
    "V7\n",
    "\n",
    "Simplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .‚Äã\n",
    "\n",
    "Increase Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .‚Äã\n",
    "\n",
    "Data Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .‚Äã\n",
    "\n",
    "Dropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Pruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .‚Äã\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1da53",
   "metadata": {},
   "source": [
    "Q4 Explain bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd34ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n+------------------+-------------------+------------------+------------------+\\nüìå What is Bias-Variance Tradeoff?\\nWhen we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\\n\\nBut sometimes, models make two types of mistakes:\\n+------------------+-------------------+------------------+------------------+\\n                                                                             |    \\nüîπ Bias (Too Simple Brain üß†)                                               |    \\nThink of it like a student who didn‚Äôt study enough.                          |   \\n\\nThey make a lot of mistakes because they don‚Äôt understand the topic well.     \\n\\nThe model is too simple and misses important patterns.\\n\\nThis is called underfitting.\\n+------------------+-------------------+------------------+------------------+\\n\\nüîπ Variance (Too Smart-for-This Brain ü§Ø)\\nNow imagine a student who memorized every single question from the textbook.\\n\\nThey do great on practice questions but struggle on the actual test because the questions are slightly different.\\n\\nThe model is too complex and overfits the training data.\\n\\nIt learns noise instead of the real pattern.\\n+------------------+-------------------+------------------+------------------+\\n\\n‚öñÔ∏è The Tradeoff\\nYou want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\\n\\nThe sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\\n\\n+------------------+-------------------+------------------+------------------+\\nüéØ Simple Analogy\\n\\nModel Type\\tLike a student who...\\tProblem\\nHigh Bias\\tDidn‚Äôt study much, guesses answers\\tUnderfitting\\nHigh Variance\\tMemorized the book, confused in the test\\tOverfitting\\nJust Right ‚úÖ\\tUnderstood the topic, answers confidently\\tGood Model!\\n+------------------+-------------------+------------------+------------------+\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üìå What is Bias-Variance Tradeoff?\n",
    "When we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\n",
    "\n",
    "But sometimes, models make two types of mistakes:\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "                                                                             |    \n",
    "üîπ Bias (Too Simple Brain üß†)                                               |    \n",
    "Think of it like a student who didn‚Äôt study enough.                          |   \n",
    "                                                                             |   \n",
    "They make a lot of mistakes because they don‚Äôt understand the topic well.    |\n",
    "The model is too simple and misses important patterns.                       |\n",
    "This is called underfitting.                                                 |   \n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "üîπ Variance (Too Smart-for-This Brain ü§Ø)\n",
    "Now imagine a student who memorized every single question from the textbook.\n",
    "\n",
    "They do great on practice questions but struggle on the actual test because the questions are slightly different.\n",
    "\n",
    "The model is too complex and overfits the training data.\n",
    "\n",
    "It learns noise instead of the real pattern.\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "‚öñÔ∏è The Tradeoff\n",
    "You want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\n",
    "\n",
    "The sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\n",
    "\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üéØ Simple Analogy\n",
    "\n",
    "Model Type\tLike a student who...\tProblem\n",
    "High Bias\tDidn‚Äôt study much, guesses answers\tUnderfitting\n",
    "High Variance\tMemorized the book, confused in the test\tOverfitting\n",
    "Just Right ‚úÖ\tUnderstood the topic, answers confidently\tGood Model!\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c55d2",
   "metadata": {},
   "source": [
    "## What are the assumptions (Aanuman lagana) of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c91634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If we assume a linear relationship between X and y, but in reality it's non-linear, then a linear model will not fit well, and the predictions will be poor.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''If we assume a linear relationship between X and y, but in reality it's non-linear, then a linear model will not fit well, and the predictions will be poor.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128c7e4",
   "metadata": {},
   "source": [
    "### Normalize and standardize features using MinMaxScaler and StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cb8ac",
   "metadata": {},
   "source": [
    "What it does: Scales all values between 0 and 1 (or a given range).\n",
    "\n",
    "Formula:\n",
    "‚Äã\n",
    " \n",
    "Use when: Data does not follow a Gaussian (normal) distribution.\n",
    "\n",
    "# Standardization (StandardScaler)\n",
    "What it does: Transforms data to have a mean = 0 and standard deviation = 1.\n",
    "\n",
    "X  = œÉ/X‚àíŒº\n",
    "‚ÄãUse when: Data is normally distributed or you want to retain outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4975a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNormalization vs Standardization:\\nNormalization (MinMaxScaler):\\nYe use karte hain jab hum chahte hain ke saare features ek common range mein ho, jaise 0 to 1. Matlab, data ko ek fixed scale pe laate hain.\\n\\nUse case: Agar aap KNN ya Neural Networks use kar rahe ho, toh normalization zaroori hota hai kyunki unke liye features ka range important hota hai. Jaise agar ek feature ka range 0 to 100 hai aur doosra ka 0 to 1, toh KNN ko distanced calculation mein problem ho sakti hai.\\n\\nExample:\\nSuppose aapke paas ek data hai:\\n\\nFeature1: [10, 20, 30, 40, 50]\\n\\nFeature2: [100, 200, 300, 400, 500]\\n\\nNormalize karne ke baad, Feature1 and Feature2 dono ko 0 to 1 ke range mein scale kar denge.\\n\\nStandardization (StandardScaler):\\nYe tab use karte hain jab hum chahte hain ke data ka mean 0 ho aur standard deviation 1 ho. Matlab, features ko normalize karte hain taki unka spread equal ho, aur kuch outliers ko bhi preserve kiya jaa sake.\\n\\nUse case: Agar aap Linear Models (jaise Logistic Regression, SVM) ya PCA use kar rahe ho, toh standardization important hota hai. Ye models assume karte hain ke data ka mean 0 aur variance 1 ho.\\n\\nExample:\\nSuppose aapke paas ek data hai:\\n\\nFeature1: [10, 20, 30, 40, 50]\\n\\nFeature2: [100, 200, 300, 400, 500]\\n\\nStandardize karne ke baad, Feature1 aur Feature2 dono ka mean 0 ho jaayega aur standard deviation 1 ho jaayegi.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Normalization vs Standardization:\n",
    "Normalization (MinMaxScaler):\n",
    "Ye use karte hain jab hum chahte hain ke saare features ek common range mein ho, jaise 0 to 1. Matlab, data ko ek fixed scale pe laate hain.\n",
    "\n",
    "Use case: Agar aap KNN ya Neural Networks use kar rahe ho, toh normalization zaroori hota hai kyunki unke liye features ka range important hota hai. Jaise agar ek feature ka range 0 to 100 hai aur doosra ka 0 to 1, toh KNN ko distanced calculation mein problem ho sakti hai.\n",
    "\n",
    "Example:\n",
    "Suppose aapke paas ek data hai:\n",
    "\n",
    "Feature1: [10, 20, 30, 40, 50]\n",
    "\n",
    "Feature2: [100, 200, 300, 400, 500]\n",
    "\n",
    "Normalize karne ke baad, Feature1 and Feature2 dono ko 0 to 1 ke range mein scale kar denge.\n",
    "\n",
    "Standardization (StandardScaler):\n",
    "Ye tab use karte hain jab hum chahte hain ke data ka mean 0 ho aur standard deviation 1 ho. Matlab, features ko normalize karte hain taki unka spread equal ho, aur kuch outliers ko bhi preserve kiya jaa sake.\n",
    "\n",
    "Use case: Agar aap Linear Models (jaise Logistic Regression, SVM) ya PCA use kar rahe ho, toh standardization important hota hai. Ye models assume karte hain ke data ka mean 0 aur variance 1 ho.\n",
    "\n",
    "Example:\n",
    "Suppose aapke paas ek data hai:\n",
    "\n",
    "Feature1: [10, 20, 30, 40, 50]\n",
    "\n",
    "Feature2: [100, 200, 300, 400, 500]\n",
    "\n",
    "Standardize karne ke baad, Feature1 aur Feature2 dono ka mean 0 ho jaayega aur standard deviation 1 ho jaayegi.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "logistic Regression is a classification algorithm, not a regression one (despite the name). It predicts probabilities for binary or multi-class outcomes using a sigmoid function.\n",
    "\n",
    "üîç How It Works (Step-by-Step)\n",
    "‚úÖ 1. Linear Combination (like Linear Regression)\n",
    "It starts with:\n",
    "\n",
    "This is a weighted sum of the inputs.\n",
    "\n",
    "‚úÖ 2. Sigmoid Function (to squash into probability)\n",
    "\n",
    " \n",
    "This converts the output to a probability between 0 and 1.\n",
    "\n",
    "‚úÖ 3. Thresholding\n",
    "If the probability is:\n",
    "\n",
    "‚â• 0.5 ‚Üí Class 1\n",
    "\n",
    "< 0.5 ‚Üí Class 0\n",
    "\n",
    "You can change the threshold based on your use case (e.g., 0.7).\n",
    "\n",
    "üéØ Goal\n",
    "Find the weights w that minimize the error (using log loss or cross-entropy):\n",
    "\n",
    "Loss\n",
    "=\n",
    "‚àí\n",
    "[\n",
    "ùë¶\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "ùë¶\n",
    "^\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùë¶\n",
    ")\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùë¶\n",
    "^\n",
    ")\n",
    "]\n",
    "Loss=‚àí[ylog( \n",
    "y\n",
    "^\n",
    "‚Äã\n",
    " )+(1‚àíy)log(1‚àí \n",
    "y\n",
    "^\n",
    "‚Äã\n",
    " )]\n",
    "‚öôÔ∏è Training (Gradient Descent)\n",
    "Compute gradients of the loss\n",
    "\n",
    "Update weights:\n",
    "\n",
    "ùë§\n",
    "=\n",
    "ùë§\n",
    "‚àí\n",
    "ùõº\n",
    "‚ãÖ\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë§\n",
    "w=w‚àíŒ±‚ãÖ \n",
    "‚àÇw\n",
    "‚àÇL\n",
    "‚Äã\n",
    " \n",
    "where Œ± is the learning rate\n",
    "\n",
    "üìå Example\n",
    "You want to classify if an email is spam or not:\n",
    "\n",
    "Feature\tEmail Length\tHas \"Free\"?\tLabel\n",
    "Email 1\t120\t1\t1\n",
    "Email 2\t60\t0\t0\n",
    "\n",
    "Model learns weights to output spam probability like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "z = 0.3*120 + 1.2*1 - 50\n",
    "p = sigmoid(z) ‚âà 0.9 ‚Üí Spam\n",
    "üß† Summary\n",
    "Used for binary/multiclass classification\n",
    "\n",
    "Uses sigmoid to produce probability\n",
    "\n",
    "Learns weights by minimizing log loss\n",
    "\n",
    "Output is a probability, not a continuous value\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe345b9a",
   "metadata": {},
   "source": [
    "## What is multicollinearity and how do you detect it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050c1ab",
   "metadata": {},
   "source": [
    "ü§î Multicollinearity kya hota hai?\n",
    "Multicollinearity tab hota hai jab 2 ya usse zyada independent variables (features) ek dusre se zyada correlated hote hain ‚Äî yaani ek feature ke value se doosre ka andaza lagaya ja sakta hai.\n",
    "\n",
    "üéØ Regression models (jaise Linear Regression) mein yeh problem hoti hai.\n",
    "üß† Isme dikkat kya hoti hai?\n",
    "Model confuse ho jata hai kis variable ka kitna impact hai target par.\n",
    "\n",
    "Coefficients unreliable ho jate hain (kabhi negative kabhi positive).\n",
    "\n",
    "Model overfit ho sakta hai, ya data thoda change hone par prediction drastically badal sakta hai.\n",
    "\n",
    "Statistical test (p-values) bekar result de sakte hain.\n",
    "\n",
    "\n",
    "üîç Multicollinearity kaise detect karte hain?\n",
    "‚úÖ 1. Correlation Matrix\n",
    "Ek heatmap banao ya correlation matrix dekho. Agar do columns ka correlation 0.8+ ya -0.8 se zyada hai, toh dikkat hai.\n",
    "\n",
    " 2. VIF (Variance Inflation Factor)\n",
    "VIF batata hai ki ek variable dusre variables se kitna affected hai.\n",
    "\n",
    "\n",
    "üîß Iska Solution kya hai?\n",
    "Ek variable hata do jo duplicate info de raha ho.\n",
    "\n",
    "Use PCA (Principal Component Analysis) for dimensionality reduction.\n",
    "\n",
    "Use Ridge/Lasso Regression ‚Äî yeh regularization techniques hai jo multicollinearity handle karti hain.\n",
    "\n",
    "üìç Multicollinearity kaha use hoti hai?\n",
    "Multicollinearity khud use nahi hoti ‚Äî yeh ek problem hai jo mainly regression models mein aati hai, jaise:\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Time Series Forecasting\n",
    "\n",
    "Econometrics models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e2f6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5712507",
   "metadata": {},
   "source": [
    "## what is l1 & l2 , where we use & what about softmax or sigmod\n",
    "\n",
    "\"L1 use karte hain jab feature selection important ho. L2 use hota hai jab overfitting control karni ho but features lose nahi karne.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07d5c4",
   "metadata": {},
   "source": [
    "üîπ L1 Regularization (Lasso)\n",
    "üìñ Definition:\n",
    "L1 regularization ek aisa technique hai jo model ke weight ke absolute value ko minimize karta hai.\n",
    "Formula: Loss + Œª * Œ£|weights|\n",
    "\n",
    "üß∞ Use:\n",
    "Jab feature selection chahiye ho (matlab kuch features important nahi hain).\n",
    "\n",
    "Ye kuch weights ko zero kar deta hai ‚Üí to model simple ho jaata hai.\n",
    "\n",
    "üó£Ô∏è Hinglish:\n",
    "L1 regularization model ke kuch weights ko zero kar deta hai, isse unimportant features remove ho jaate hain. Ye sparse model banata hai jo fast aur easy to interpret hota hai.\n",
    "\n",
    "üîπ L2 Regularization (Ridge)\n",
    "üìñ Definition:\n",
    "L2 regularization ek technique hai jo weights ke square ko minimize karta hai.\n",
    "Formula: Loss + Œª * Œ£(weights¬≤)\n",
    "\n",
    "üß∞ Use:\n",
    "Jab aap overfitting se bachna chahte ho aur saare features rakhne hain.\n",
    "\n",
    "Weights ko chhota karta hai, lekin zero nahi karta.\n",
    "\n",
    "üó£Ô∏è Hinglish:\n",
    "L2 regularization model ke sabhi weights ko thoda-thoda shrink karta hai, isse overfitting kam hoti hai. Ye saare features ko rakhta hai, lekin unka effect control karta hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e9452",
   "metadata": {},
   "source": [
    "### Activation fucn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2e61c",
   "metadata": {},
   "source": [
    "ReLU bas positive values ko pass karta hai, aur negative values ko zero kar deta hai. Ye fast aur simple hota hai, isi liye hidden layers me use karte hain.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "Sigmoid ek activation function hai jo kisi bhi value ko compress karke 0 se 1 ke beech me le aata hai.\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "Softmax bhi ek activation function hai jo multiple classes ke liye kaam karta hai. Ye har class ke liye probability nikalta hai, aur sabka sum 1 hota hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225371cf",
   "metadata": {},
   "source": [
    "‚úÖ ReLU, Sigmoid & Softmax ‚Äî Ye kaha use hote hain?\n",
    "üî∏ 1. ReLU (Rectified Linear Unit)\n",
    "üìç Kaha use hota hai?\n",
    "‚û°Ô∏è Hidden layers me.\n",
    "\n",
    "üìå Use kyun karte hain?\n",
    "\n",
    "Non-linearity add karta hai\n",
    "\n",
    "Fast training\n",
    "\n",
    "Gradient vanishing problem solve karta hai (better than sigmoid/tanh)\n",
    "\n",
    "üìú Formula:\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "üó£Ô∏è Hinglish me:\n",
    "\n",
    "ReLU mostly hidden layers me use hota hai taaki network me non-linearity aaye aur fast training ho. Zero ke niche sab kuch cut ho jaata hai.\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "üî∏ 2. Sigmoid\n",
    "üìç Kaha use hota hai?\n",
    "‚û°Ô∏è Output layer me, binary classification ke liye.\n",
    "\n",
    "üìå Use kyun karte hain?\n",
    "\n",
    "Output between 0 and 1\n",
    "\n",
    "Probability dena ho to perfect\n",
    "\n",
    "üìú Formula:\n",
    "Sigmoid(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "üó£Ô∏è Hinglish me:\n",
    "\n",
    "Jab model ko decide karna ho ki class 0 ya 1 (binary), tab sigmoid output layer me lagate hain.\n",
    "\n",
    "üß† Example:\n",
    "Tumhe batana ho ki image me cat hai ya nahi, to output layer me sigmoid use hoga.\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "üî∏ 3. Softmax\n",
    "üìç Kaha use hota hai?\n",
    "‚û°Ô∏è Output layer, multi-class classification me.\n",
    "\n",
    "üìå Use kyun karte hain?\n",
    "\n",
    "Sab class ki probability deta hai\n",
    "\n",
    "Sum = 1 hota hai (probability distribution)\n",
    "\n",
    "üìú Formula:\n",
    "softmax(x·µ¢) = eÀ£‚Å± / Œ£eÀ£\n",
    "\n",
    "üó£Ô∏è Hinglish me:\n",
    "\n",
    "Jab 3 ya 4 se zyada classes ho (e.g., dog, cat, cow, elephant), tab softmax use hota hai output layer me ‚Äî jo batata hai ki kis class ki probability kitni hai.\n",
    "\n",
    "üß† Example:\n",
    "Image me kya hai?\n",
    "\n",
    "Dog: 0.01\n",
    "\n",
    "Cat: 0.97\n",
    "\n",
    "Cow: 0.01\n",
    "\n",
    "Elephant: 0.01\n",
    "\n",
    "Yeh output softmax se aata hai.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488c7cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
