{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845f804a",
   "metadata": {},
   "source": [
    "Q1 : What is the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ac78ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDefinition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.\\u200b\\n\\nKey Characteristics:\\n\\nUnlabeled Data: Works with data that has no predefined labels or categories.\\n\\nObjective: Discover hidden patterns, groupings, or features in the data.\\n\\nCommon Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.\\u200b\\n\\nExamples:\\n\\nCustomer Segmentation: Group customers based on purchasing behavior without predefined categories.\\n\\nAnomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\\n\\nMarket Basket Analysis: Discover associations between products bought together.\\u200b\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#üìò Supervised Learning\n",
    "'''Definition: Supervised learning involves training a model on a labeled dataset, where each input is paired with the correct output. The model learns to predict the output from the input data.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Labeled Data: Requires a dataset with input-output pairs.\n",
    "\n",
    "Objective: Learn a mapping from inputs to outputs to make accurate predictions on new, unseen data.\n",
    "\n",
    "Common Algorithms: Linear regression, logistic regression, decision trees, support vector machines, neural networks.‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Examples:\n",
    "\n",
    "Email Spam Detection: Classify emails as 'spam' or 'not spam' based on labeled examples.\n",
    "\n",
    "Medical Diagnosis: Predict disease presence from patient data with known diagnoses.\n",
    "\n",
    "Credit Scoring: Assess loan applicants' creditworthiness using historical data with known outcomes '''\n",
    "\n",
    "\n",
    "# üìô Unsupervised Learning\n",
    "'''\n",
    "Definition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Unlabeled Data: Works with data that has no predefined labels or categories.\n",
    "\n",
    "Objective: Discover hidden patterns, groupings, or features in the data.\n",
    "\n",
    "Common Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.‚Äã\n",
    "\n",
    "Examples:\n",
    "\n",
    "Customer Segmentation: Group customers based on purchasing behavior without predefined categories.\n",
    "\n",
    "Anomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\n",
    "\n",
    "Market Basket Analysis: Discover associations between products bought together.‚Äã\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72a4c3",
   "metadata": {},
   "source": [
    "What is overfitting? How can you prevent it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d137fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \\nStack Overflow\\n.\\u200b\\n\\n‚úÖ How to Prevent Overfitting\\nTo mitigate overfitting, consider the following strategies:\\n\\nCross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .\\u200b\\nEliteDataScience\\n\\nRegularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .\\u200b\\n\\nEarly Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .\\u200b\\nV7\\n\\nSimplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .\\u200b\\n\\nIncrease Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .\\u200b\\n\\nData Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .\\u200b\\n\\nDropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .\\u200b\\nWikipedia\\n\\nPruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .\\u200b\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \n",
    "Stack Overflow\n",
    ".‚Äã\n",
    "\n",
    "‚úÖ How to Prevent Overfitting\n",
    "To mitigate overfitting, consider the following strategies:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .‚Äã\n",
    "EliteDataScience\n",
    "\n",
    "Regularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .‚Äã\n",
    "\n",
    "Early Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .‚Äã\n",
    "V7\n",
    "\n",
    "Simplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .‚Äã\n",
    "\n",
    "Increase Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .‚Äã\n",
    "\n",
    "Data Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .‚Äã\n",
    "\n",
    "Dropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Pruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .‚Äã\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1da53",
   "metadata": {},
   "source": [
    "Q4 Explain bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd34ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n+------------------+-------------------+------------------+------------------+\\nüìå What is Bias-Variance Tradeoff?\\nWhen we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\\n\\nBut sometimes, models make two types of mistakes:\\n+------------------+-------------------+------------------+------------------+\\n                                                                             |    \\nüîπ Bias (Too Simple Brain üß†)                                               |    \\nThink of it like a student who didn‚Äôt study enough.                          |   \\n\\nThey make a lot of mistakes because they don‚Äôt understand the topic well.     \\n\\nThe model is too simple and misses important patterns.\\n\\nThis is called underfitting.\\n+------------------+-------------------+------------------+------------------+\\n\\nüîπ Variance (Too Smart-for-This Brain ü§Ø)\\nNow imagine a student who memorized every single question from the textbook.\\n\\nThey do great on practice questions but struggle on the actual test because the questions are slightly different.\\n\\nThe model is too complex and overfits the training data.\\n\\nIt learns noise instead of the real pattern.\\n+------------------+-------------------+------------------+------------------+\\n\\n‚öñÔ∏è The Tradeoff\\nYou want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\\n\\nThe sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\\n\\n+------------------+-------------------+------------------+------------------+\\nüéØ Simple Analogy\\n\\nModel Type\\tLike a student who...\\tProblem\\nHigh Bias\\tDidn‚Äôt study much, guesses answers\\tUnderfitting\\nHigh Variance\\tMemorized the book, confused in the test\\tOverfitting\\nJust Right ‚úÖ\\tUnderstood the topic, answers confidently\\tGood Model!\\n+------------------+-------------------+------------------+------------------+\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üìå What is Bias-Variance Tradeoff?\n",
    "When we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\n",
    "\n",
    "But sometimes, models make two types of mistakes:\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "                                                                             |    \n",
    "üîπ Bias (Too Simple Brain üß†)                                               |    \n",
    "Think of it like a student who didn‚Äôt study enough.                          |   \n",
    "                                                                             |   \n",
    "They make a lot of mistakes because they don‚Äôt understand the topic well.    |\n",
    "The model is too simple and misses important patterns.                       |\n",
    "This is called underfitting.                                                 |   \n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "üîπ Variance (Too Smart-for-This Brain ü§Ø)\n",
    "Now imagine a student who memorized every single question from the textbook.\n",
    "\n",
    "They do great on practice questions but struggle on the actual test because the questions are slightly different.\n",
    "\n",
    "The model is too complex and overfits the training data.\n",
    "\n",
    "It learns noise instead of the real pattern.\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "‚öñÔ∏è The Tradeoff\n",
    "You want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\n",
    "\n",
    "The sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\n",
    "\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üéØ Simple Analogy\n",
    "\n",
    "Model Type\tLike a student who...\tProblem\n",
    "High Bias\tDidn‚Äôt study much, guesses answers\tUnderfitting\n",
    "High Variance\tMemorized the book, confused in the test\tOverfitting\n",
    "Just Right ‚úÖ\tUnderstood the topic, answers confidently\tGood Model!\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c55d2",
   "metadata": {},
   "source": [
    "## What are the assumptions (Aanuman lagana) of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c91634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If we assume a linear relationship between X and y, but in reality it's non-linear, then a linear model will not fit well, and the predictions will be poor.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''If we assume a linear relationship between X and y, but in reality it's non-linear, then a linear model will not fit well, and the predictions will be poor.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128c7e4",
   "metadata": {},
   "source": [
    "### Normalize and standardize features using MinMaxScaler and StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cb8ac",
   "metadata": {},
   "source": [
    "What it does: Scales all values between 0 and 1 (or a given range).\n",
    "\n",
    "Formula:\n",
    "‚Äã\n",
    " \n",
    "Use when: Data does not follow a Gaussian (normal) distribution.\n",
    "\n",
    "# Standardization (StandardScaler)\n",
    "What it does: Transforms data to have a mean = 0 and standard deviation = 1.\n",
    "\n",
    "X  = œÉ/X‚àíŒº\n",
    "‚ÄãUse when: Data is normally distributed or you want to retain outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4975a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNormalization vs Standardization:\\nNormalization (MinMaxScaler):\\nYe use karte hain jab hum chahte hain ke saare features ek common range mein ho, jaise 0 to 1. Matlab, data ko ek fixed scale pe laate hain.\\n\\nUse case: Agar aap KNN ya Neural Networks use kar rahe ho, toh normalization zaroori hota hai kyunki unke liye features ka range important hota hai. Jaise agar ek feature ka range 0 to 100 hai aur doosra ka 0 to 1, toh KNN ko distanced calculation mein problem ho sakti hai.\\n\\nExample:\\nSuppose aapke paas ek data hai:\\n\\nFeature1: [10, 20, 30, 40, 50]\\n\\nFeature2: [100, 200, 300, 400, 500]\\n\\nNormalize karne ke baad, Feature1 and Feature2 dono ko 0 to 1 ke range mein scale kar denge.\\n\\nStandardization (StandardScaler):\\nYe tab use karte hain jab hum chahte hain ke data ka mean 0 ho aur standard deviation 1 ho. Matlab, features ko normalize karte hain taki unka spread equal ho, aur kuch outliers ko bhi preserve kiya jaa sake.\\n\\nUse case: Agar aap Linear Models (jaise Logistic Regression, SVM) ya PCA use kar rahe ho, toh standardization important hota hai. Ye models assume karte hain ke data ka mean 0 aur variance 1 ho.\\n\\nExample:\\nSuppose aapke paas ek data hai:\\n\\nFeature1: [10, 20, 30, 40, 50]\\n\\nFeature2: [100, 200, 300, 400, 500]\\n\\nStandardize karne ke baad, Feature1 aur Feature2 dono ka mean 0 ho jaayega aur standard deviation 1 ho jaayegi.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Normalization vs Standardization:\n",
    "Normalization (MinMaxScaler):\n",
    "Ye use karte hain jab hum chahte hain ke saare features ek common range mein ho, jaise 0 to 1. Matlab, data ko ek fixed scale pe laate hain.\n",
    "\n",
    "Use case: Agar aap KNN ya Neural Networks use kar rahe ho, toh normalization zaroori hota hai kyunki unke liye features ka range important hota hai. Jaise agar ek feature ka range 0 to 100 hai aur doosra ka 0 to 1, toh KNN ko distanced calculation mein problem ho sakti hai.\n",
    "\n",
    "Example:\n",
    "Suppose aapke paas ek data hai:\n",
    "\n",
    "Feature1: [10, 20, 30, 40, 50]\n",
    "\n",
    "Feature2: [100, 200, 300, 400, 500]\n",
    "\n",
    "Normalize karne ke baad, Feature1 and Feature2 dono ko 0 to 1 ke range mein scale kar denge.\n",
    "\n",
    "Standardization (StandardScaler):\n",
    "Ye tab use karte hain jab hum chahte hain ke data ka mean 0 ho aur standard deviation 1 ho. Matlab, features ko normalize karte hain taki unka spread equal ho, aur kuch outliers ko bhi preserve kiya jaa sake.\n",
    "\n",
    "Use case: Agar aap Linear Models (jaise Logistic Regression, SVM) ya PCA use kar rahe ho, toh standardization important hota hai. Ye models assume karte hain ke data ka mean 0 aur variance 1 ho.\n",
    "\n",
    "Example:\n",
    "Suppose aapke paas ek data hai:\n",
    "\n",
    "Feature1: [10, 20, 30, 40, 50]\n",
    "\n",
    "Feature2: [100, 200, 300, 400, 500]\n",
    "\n",
    "Standardize karne ke baad, Feature1 aur Feature2 dono ka mean 0 ho jaayega aur standard deviation 1 ho jaayegi.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "logistic Regression is a classification algorithm, not a regression one (despite the name). It predicts probabilities for binary or multi-class outcomes using a sigmoid function.\n",
    "\n",
    "üîç How It Works (Step-by-Step)\n",
    "‚úÖ 1. Linear Combination (like Linear Regression)\n",
    "It starts with:\n",
    "\n",
    "This is a weighted sum of the inputs.\n",
    "\n",
    "‚úÖ 2. Sigmoid Function (to squash into probability)\n",
    "\n",
    " \n",
    "This converts the output to a probability between 0 and 1.\n",
    "\n",
    "‚úÖ 3. Thresholding\n",
    "If the probability is:\n",
    "\n",
    "‚â• 0.5 ‚Üí Class 1\n",
    "\n",
    "< 0.5 ‚Üí Class 0\n",
    "\n",
    "You can change the threshold based on your use case (e.g., 0.7).\n",
    "\n",
    "üéØ Goal\n",
    "Find the weights w that minimize the error (using log loss or cross-entropy):\n",
    "\n",
    "Loss\n",
    "=\n",
    "‚àí\n",
    "[\n",
    "ùë¶\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "ùë¶\n",
    "^\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùë¶\n",
    ")\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùë¶\n",
    "^\n",
    ")\n",
    "]\n",
    "Loss=‚àí[ylog( \n",
    "y\n",
    "^\n",
    "‚Äã\n",
    " )+(1‚àíy)log(1‚àí \n",
    "y\n",
    "^\n",
    "‚Äã\n",
    " )]\n",
    "‚öôÔ∏è Training (Gradient Descent)\n",
    "Compute gradients of the loss\n",
    "\n",
    "Update weights:\n",
    "\n",
    "ùë§\n",
    "=\n",
    "ùë§\n",
    "‚àí\n",
    "ùõº\n",
    "‚ãÖ\n",
    "‚àÇ\n",
    "ùêø\n",
    "‚àÇ\n",
    "ùë§\n",
    "w=w‚àíŒ±‚ãÖ \n",
    "‚àÇw\n",
    "‚àÇL\n",
    "‚Äã\n",
    " \n",
    "where Œ± is the learning rate\n",
    "\n",
    "üìå Example\n",
    "You want to classify if an email is spam or not:\n",
    "\n",
    "Feature\tEmail Length\tHas \"Free\"?\tLabel\n",
    "Email 1\t120\t1\t1\n",
    "Email 2\t60\t0\t0\n",
    "\n",
    "Model learns weights to output spam probability like:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "z = 0.3*120 + 1.2*1 - 50\n",
    "p = sigmoid(z) ‚âà 0.9 ‚Üí Spam\n",
    "üß† Summary\n",
    "Used for binary/multiclass classification\n",
    "\n",
    "Uses sigmoid to produce probability\n",
    "\n",
    "Learns weights by minimizing log loss\n",
    "\n",
    "Output is a probability, not a continuous value\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe345b9a",
   "metadata": {},
   "source": [
    "## What is multicollinearity and how do you detect it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050c1ab",
   "metadata": {},
   "source": [
    "ü§î Multicollinearity kya hota hai?\n",
    "Multicollinearity tab hota hai jab 2 ya usse zyada independent variables (features) ek dusre se zyada correlated hote hain ‚Äî yaani ek feature ke value se doosre ka andaza lagaya ja sakta hai.\n",
    "\n",
    "üéØ Regression models (jaise Linear Regression) mein yeh problem hoti hai.\n",
    "üß† Isme dikkat kya hoti hai?\n",
    "Model confuse ho jata hai kis variable ka kitna impact hai target par.\n",
    "\n",
    "Coefficients unreliable ho jate hain (kabhi negative kabhi positive).\n",
    "\n",
    "Model overfit ho sakta hai, ya data thoda change hone par prediction drastically badal sakta hai.\n",
    "\n",
    "Statistical test (p-values) bekar result de sakte hain.\n",
    "\n",
    "\n",
    "üîç Multicollinearity kaise detect karte hain?\n",
    "‚úÖ 1. Correlation Matrix\n",
    "Ek heatmap banao ya correlation matrix dekho. Agar do columns ka correlation 0.8+ ya -0.8 se zyada hai, toh dikkat hai.\n",
    "\n",
    " 2. VIF (Variance Inflation Factor)\n",
    "VIF batata hai ki ek variable dusre variables se kitna affected hai.\n",
    "\n",
    "\n",
    "üîß Iska Solution kya hai?\n",
    "Ek variable hata do jo duplicate info de raha ho.\n",
    "\n",
    "Use PCA (Principal Component Analysis) for dimensionality reduction.\n",
    "\n",
    "Use Ridge/Lasso Regression ‚Äî yeh regularization techniques hai jo multicollinearity handle karti hain.\n",
    "\n",
    "üìç Multicollinearity kaha use hoti hai?\n",
    "Multicollinearity khud use nahi hoti ‚Äî yeh ek problem hai jo mainly regression models mein aati hai, jaise:\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Time Series Forecasting\n",
    "\n",
    "Econometrics models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e2f6a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
