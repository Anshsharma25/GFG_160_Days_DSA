{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845f804a",
   "metadata": {},
   "source": [
    "Q1 : What is the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ac78ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDefinition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.\\u200b\\n\\nKey Characteristics:\\n\\nUnlabeled Data: Works with data that has no predefined labels or categories.\\n\\nObjective: Discover hidden patterns, groupings, or features in the data.\\n\\nCommon Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.\\u200b\\n\\nExamples:\\n\\nCustomer Segmentation: Group customers based on purchasing behavior without predefined categories.\\n\\nAnomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\\n\\nMarket Basket Analysis: Discover associations between products bought together.\\u200b\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#üìò Supervised Learning\n",
    "'''Definition: Supervised learning involves training a model on a labeled dataset, where each input is paired with the correct output. The model learns to predict the output from the input data.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Labeled Data: Requires a dataset with input-output pairs.\n",
    "\n",
    "Objective: Learn a mapping from inputs to outputs to make accurate predictions on new, unseen data.\n",
    "\n",
    "Common Algorithms: Linear regression, logistic regression, decision trees, support vector machines, neural networks.‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Examples:\n",
    "\n",
    "Email Spam Detection: Classify emails as 'spam' or 'not spam' based on labeled examples.\n",
    "\n",
    "Medical Diagnosis: Predict disease presence from patient data with known diagnoses.\n",
    "\n",
    "Credit Scoring: Assess loan applicants' creditworthiness using historical data with known outcomes '''\n",
    "\n",
    "\n",
    "# üìô Unsupervised Learning\n",
    "'''\n",
    "Definition: Unsupervised learning deals with unlabeled data. The model tries to learn the underlying structure or distribution in the data without explicit instructions.‚Äã\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Unlabeled Data: Works with data that has no predefined labels or categories.\n",
    "\n",
    "Objective: Discover hidden patterns, groupings, or features in the data.\n",
    "\n",
    "Common Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders.‚Äã\n",
    "\n",
    "Examples:\n",
    "\n",
    "Customer Segmentation: Group customers based on purchasing behavior without predefined categories.\n",
    "\n",
    "Anomaly Detection: Identify unusual patterns or outliers in data, such as fraudulent transactions.\n",
    "\n",
    "Market Basket Analysis: Discover associations between products bought together.‚Äã\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72a4c3",
   "metadata": {},
   "source": [
    "What is overfitting? How can you prevent it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d137fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \\nStack Overflow\\n.\\u200b\\n\\n‚úÖ How to Prevent Overfitting\\nTo mitigate overfitting, consider the following strategies:\\n\\nCross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .\\u200b\\nEliteDataScience\\n\\nRegularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .\\u200b\\n\\nEarly Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .\\u200b\\nV7\\n\\nSimplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .\\u200b\\n\\nIncrease Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .\\u200b\\n\\nData Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .\\u200b\\n\\nDropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .\\u200b\\nWikipedia\\n\\nPruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .\\u200b\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In overfitting, a model achieves high accuracy on training data but fails to perform well on validation or test data. This indicates that the model has memorized the training data rather than learning the underlying patterns, resulting in low bias but high variance \n",
    "Stack Overflow\n",
    ".‚Äã\n",
    "\n",
    "‚úÖ How to Prevent Overfitting\n",
    "To mitigate overfitting, consider the following strategies:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on different subsets of data, ensuring it generalizes well .‚Äã\n",
    "EliteDataScience\n",
    "\n",
    "Regularization: Apply penalties to model complexity through methods like L1 (Lasso) and L2 (Ridge) regularization, which discourage overly complex models .‚Äã\n",
    "\n",
    "Early Stopping: Monitor the model's performance on a validation set during training and halt training when performance begins to degrade, preventing the model from learning noise .‚Äã\n",
    "V7\n",
    "\n",
    "Simplify the Model: Reduce the number of features or parameters to decrease model complexity, making it less likely to overfit .‚Äã\n",
    "\n",
    "Increase Training Data: Providing more diverse and representative data can help the model learn general patterns rather than memorizing the training set .‚Äã\n",
    "\n",
    "Data Augmentation: For tasks like image recognition, augmenting data by applying transformations (e.g., rotations, scaling) can increase dataset diversity without collecting new data .‚Äã\n",
    "\n",
    "Dropout: In neural networks, randomly dropping units during training prevents units from co-adapting too much, promoting robustness .‚Äã\n",
    "Wikipedia\n",
    "\n",
    "Pruning: Remove parts of the model that contribute little to output, reducing complexity and overfitting risk .‚Äã\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1da53",
   "metadata": {},
   "source": [
    "Q4 Explain bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fd34ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n+------------------+-------------------+------------------+------------------+\\nüìå What is Bias-Variance Tradeoff?\\nWhen we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\\n\\nBut sometimes, models make two types of mistakes:\\n+------------------+-------------------+------------------+------------------+\\n\\nüîπ Bias (Too Simple Brain üß†)\\nThink of it like a student who didn‚Äôt study enough.\\n\\nThey make a lot of mistakes because they don‚Äôt understand the topic well.\\n\\nThe model is too simple and misses important patterns.\\n\\nThis is called underfitting.\\n+------------------+-------------------+------------------+------------------+\\n\\nüîπ Variance (Too Smart-for-This Brain ü§Ø)\\nNow imagine a student who memorized every single question from the textbook.\\n\\nThey do great on practice questions but struggle on the actual test because the questions are slightly different.\\n\\nThe model is too complex and overfits the training data.\\n\\nIt learns noise instead of the real pattern.\\n+------------------+-------------------+------------------+------------------+\\n\\n‚öñÔ∏è The Tradeoff\\nYou want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\\n\\nThe sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\\n\\n+------------------+-------------------+------------------+------------------+\\nüéØ Simple Analogy\\n\\nModel Type\\tLike a student who...\\tProblem\\nHigh Bias\\tDidn‚Äôt study much, guesses answers\\tUnderfitting\\nHigh Variance\\tMemorized the book, confused in the test\\tOverfitting\\nJust Right ‚úÖ\\tUnderstood the topic, answers confidently\\tGood Model!\\n+------------------+-------------------+------------------+------------------+\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üìå What is Bias-Variance Tradeoff?\n",
    "When we train a machine learning model, we want it to learn from the data and also make good predictions on new, unseen data.\n",
    "\n",
    "But sometimes, models make two types of mistakes:\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "üîπ Bias (Too Simple Brain üß†)\n",
    "Think of it like a student who didn‚Äôt study enough.\n",
    "\n",
    "They make a lot of mistakes because they don‚Äôt understand the topic well.\n",
    "\n",
    "The model is too simple and misses important patterns.\n",
    "\n",
    "This is called underfitting.\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "üîπ Variance (Too Smart-for-This Brain ü§Ø)\n",
    "Now imagine a student who memorized every single question from the textbook.\n",
    "\n",
    "They do great on practice questions but struggle on the actual test because the questions are slightly different.\n",
    "\n",
    "The model is too complex and overfits the training data.\n",
    "\n",
    "It learns noise instead of the real pattern.\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "\n",
    "‚öñÔ∏è The Tradeoff\n",
    "You want a model that‚Äôs not too simple (low bias) and not too complex (low variance).\n",
    "\n",
    "The sweet spot is a model that understands the data well but doesn‚Äôt memorize it ‚Äî just like a student who studied smart and gets the idea.\n",
    "\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "üéØ Simple Analogy\n",
    "\n",
    "Model Type\tLike a student who...\tProblem\n",
    "High Bias\tDidn‚Äôt study much, guesses answers\tUnderfitting\n",
    "High Variance\tMemorized the book, confused in the test\tOverfitting\n",
    "Just Right ‚úÖ\tUnderstood the topic, answers confidently\tGood Model!\n",
    "+------------------+-------------------+------------------+------------------+\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f420e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
